{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs,make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3\n",
    "hidden = [4,3]\n",
    "output_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    e_pa = np.exp(a)\n",
    "    return e_pa/(np.sum(e_pa,axis = 1,keepdims = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkMock:\n",
    "    \n",
    "    def __init__(self,input_size,hidden,output_size):\n",
    "        \n",
    "        model = {}\n",
    "        \n",
    "        model['w1'] = np.random.randn(input_size,hidden[0]) \n",
    "        model['b1'] = np.zeros((1,hidden[0]))\n",
    "        \n",
    "        model['w2'] = np.random.randn(hidden[0],hidden[1])\n",
    "        model['b2'] = np.zeros((1,hidden[1]))\n",
    "        \n",
    "        model['w3'] = np.random.randn(hidden[1],output_size)\n",
    "        model['b3'] = np.zeros((1,output_size))\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        w1,w2,w3 = self.model['w1'],self.model['w2'],self.model['w3']\n",
    "        b1,b2,b3 = self.model['b1'],self.model['b2'],self.model['b3']\n",
    "        \n",
    "        z1 = np.dot(x,w1) + b1\n",
    "        a1 = np.tanh(z1)\n",
    "        \n",
    "        z2 = np.dot(a1,w2) + b2\n",
    "        a2 = np.tanh(z2)\n",
    "        \n",
    "        z3 = np.dot(a2,w3) + b3\n",
    "        y_ = softmax(z3)\n",
    "        \n",
    "        self.activation_outputs = (a1,a2,y_)\n",
    "        return y_\n",
    "    \n",
    "    def backward(self,x,y,learning_rate = 0.01):\n",
    "        \n",
    "        w1,w2,w3 = self.model['w1'],self.model['w2'],self.model['w3']\n",
    "        b1,b2,b3 = self.model['b1'],self.model['b2'],self.model['b3']\n",
    "        \n",
    "        m = x.shape[0]\n",
    "        delta3 = y_ - y\n",
    "        dw3 = np.dot(a2.T,delta3)\n",
    "        db3 = np.sum(delta3,axis=0)/float(m)\n",
    "        \n",
    "        delta2 = np.dot(delta3,w3.T)*(1-np.square(a2))\n",
    "        dw2 = np.dot(a1.T,delta2)\n",
    "        db2 = np.sum(delta2,axis=0)/float(m)\n",
    "        \n",
    "        delta1 = np.dot(delta2,w2.T)*(1-np.square(a1))\n",
    "        dw1 = np.dot(x.T,delta1)\n",
    "        db1 = np.dot(delta1,axis=0)/float(m)\n",
    "        \n",
    "        self.model['w1'] = self.model['w1'] - learning_rate*dw1\n",
    "        self.model['b1'] = self.model['b1'] - learning_rate*db1\n",
    "        \n",
    "        self.model['w2'] = self.model['w2'] - learning_rate*dw2\n",
    "        self.model['b2'] = self.model['b2'] - learning_rate*db2\n",
    "        \n",
    "        self.model['w3'] = self.model['w3'] - learning_rate*dw3\n",
    "        self.model['b3'] = self.model['b3'] - learning_rate*db3\n",
    "        \n",
    "    def predict(self,x):\n",
    "        y_ = self.forward(x)\n",
    "        return np.argmax(y_,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
